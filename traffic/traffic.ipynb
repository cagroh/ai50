{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3162626b-5311-45ae-b621-8c50d16a4c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\Anaconda3\\envs\\ai50\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 16.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 2s 136ms/step - loss: 33.6659 - accuracy: 0.5298\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 2s 133ms/step - loss: 0.2640 - accuracy: 0.9107\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 2s 138ms/step - loss: 0.0429 - accuracy: 0.9861\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 2s 135ms/step - loss: 0.0134 - accuracy: 0.9960\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 2s 129ms/step - loss: 0.0090 - accuracy: 0.9980\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 1.2557e-04 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 2s 130ms/step - loss: 3.8893e-05 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 2s 130ms/step - loss: 2.0225e-05 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 2s 133ms/step - loss: 2.0858e-05 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 2s 132ms/step - loss: 1.6880e-05 - accuracy: 1.0000\n",
      "11/11 - 0s - loss: 2.9404e-05 - accuracy: 1.0000\n",
      "WARNING:tensorflow:From D:\\Documents\\Anaconda3\\envs\\ai50\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From D:\\Documents\\Anaconda3\\envs\\ai50\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: model.sav\\assets\n",
      "Model saved to model.sav.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "EPOCHS = 10\n",
    "IMG_WIDTH = 30\n",
    "IMG_HEIGHT = 30\n",
    "NUM_CATEGORIES = 3\n",
    "TEST_SIZE = 0.4\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Check command-line arguments\n",
    "    #if len(sys.argv) not in [2, 3]:\n",
    "    #    sys.exit(\"Usage: python traffic.py data_directory [model.h5]\")\n",
    "\n",
    "    # Get image arrays and labels for all image files\n",
    "    #images, labels = load_data(sys.argv[1])\n",
    "    sys_argv1 = \"gtsrb-small\"\n",
    "    images, labels = load_data(sys_argv1)\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    labels = tf.keras.utils.to_categorical(labels)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        np.array(images), np.array(labels), test_size=TEST_SIZE\n",
    "    )\n",
    "\n",
    "    # Get a compiled neural network\n",
    "    model = get_model()\n",
    "\n",
    "    # Fit model on training data\n",
    "    model.fit(x_train, y_train, epochs=EPOCHS)\n",
    "\n",
    "    # Evaluate neural network performance\n",
    "    model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "    # Save model to file\n",
    "    #if len(sys.argv) == 3:\n",
    "    #    filename = sys.argv[2]\n",
    "    #    model.save(filename)\n",
    "    #    print(f\"Model saved to {filename}.\")\n",
    "    sys_argv2 = \"model.sav\"\n",
    "    filename = sys_argv2\n",
    "    model.save(filename)\n",
    "    print(f\"Model saved to {filename}.\")\n",
    "\n",
    "\n",
    "def load_data(data_dir):\n",
    "    \"\"\"\n",
    "    Load image data from directory `data_dir`.\n",
    "\n",
    "    Assume `data_dir` has one directory named after each category, numbered\n",
    "    0 through NUM_CATEGORIES - 1. Inside each category directory will be some\n",
    "    number of image files.\n",
    "\n",
    "    Return tuple `(images, labels)`. `images` should be a list of all\n",
    "    of the images in the data directory, where each image is formatted as a\n",
    "    numpy ndarray with dimensions IMG_WIDTH x IMG_HEIGHT x 3. `labels` should\n",
    "    be a list of integer labels, representing the categories for each of the\n",
    "    corresponding `images`.\n",
    "    \"\"\"\n",
    "    # CG: initiate resulting lists:\n",
    "    images = list()\n",
    "    labels = list()\n",
    "    filenames = list()\n",
    "    imagelist = list()\n",
    "    badreads  = list()\n",
    "\n",
    "    #CG: assume that data_dir will contain one directory named after each category, numbered 0 through NUM_CATEGORIES - 1:\n",
    "    for cat in tqdm(range(NUM_CATEGORIES)):\n",
    "\n",
    "        # CG: build search_dir by adding the category number to the starting dir:\n",
    "        search_dir = data_dir + os.sep + str(cat) + os.sep\n",
    "\n",
    "        # CG: walk down the directory tree:\n",
    "        for root, dirs, files in os.walk(search_dir):\n",
    "\n",
    "            # CG: get a list of all files in the directory:\n",
    "            names = [(os.path.join(root,f)) for f in files]\n",
    "\n",
    "            # CG: ignore empty entries:\n",
    "            if len(names) == 0:\n",
    "                continue\n",
    "\n",
    "            # CG: add an entry in the arrays for each file found:\n",
    "            for f in names:\n",
    "\n",
    "                # CG: load the image file:\n",
    "                img = cv2.imread (f, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "                # CG: if failed, add to badreads list: \n",
    "                if img is None:\n",
    "                    badreads.append (f)\n",
    "                    continue\n",
    "\n",
    "                # CG: add filename to list of valid files:\n",
    "                filenames.append(f)\n",
    "\n",
    "                # CG: resize the image to the desired sizes:\n",
    "                img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
    "\n",
    "                # CG: convert the image to RGB color scheme:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                # CG: add the image to the list:\n",
    "                images.append(img)\n",
    "\n",
    "                # CG: add the corresponding categorie to the list:\n",
    "                labels.append(cat)\n",
    "\n",
    "    # CG: return the lists:\n",
    "    return (images, labels)\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    \"\"\"\n",
    "    Returns a compiled convolutional neural network model. Assume that the\n",
    "    `input_shape` of the first layer is `(IMG_WIDTH, IMG_HEIGHT, 3)`.\n",
    "    The output layer should have `NUM_CATEGORIES` units, one for each category.\n",
    "    \"\"\"\n",
    "    \n",
    "    # CG: create the model as Sequential:\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # CG: add a convolutional layer. Learn 32 filters using a 4x4 kernel, shaping input according to given image parameters:\n",
    "    model.add (tf.keras.layers.Conv2D(32, (4, 4), activation=\"relu\", input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)))\n",
    "\n",
    "    # CG: add a convolutional layer. Learn 32 filters using a 2x2 kernel:\n",
    "    model.add (tf.keras.layers.Conv2D(32, (2, 2), activation=\"relu\"))\n",
    "\n",
    "    # CG: max-pooling layer, using 2x2 pool size:\n",
    "    model.add (tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # CG: flatten units:\n",
    "    model.add (tf.keras.layers.Flatten())\n",
    "\n",
    "    # CG: add a hidden layer with 32X32X3 (3072) neurons and 1/3% dropout:\n",
    "    model.add (tf.keras.layers.Dense(3072, activation=\"relu\"))\n",
    "    model.add (tf.keras.layers.Dropout(1/2))\n",
    "\n",
    "    # CG: add a hidden layer with 1024 neurons:\n",
    "    model.add (tf.keras.layers.Dense(1024, activation=\"relu\"))\n",
    "\n",
    "    # CG: add a hidden layer with 512 neurons:\n",
    "    model.add (tf.keras.layers.Dense(512, activation=\"relu\"))\n",
    "\n",
    "    # CG: add a hidden layer with 256:\n",
    "    model.add (tf.keras.layers.Dense(256, activation=\"relu\"))\n",
    "\n",
    "    # CG: add a hidden layer with 128:\n",
    "    model.add (tf.keras.layers.Dense(128, activation=\"relu\"))\n",
    "    #model.add (tf.keras.layers.Dropout(1/3))\n",
    "\n",
    "    # CG: add a hidden layer with 64:\n",
    "    model.add (tf.keras.layers.Dense(64, activation=\"relu\"))\n",
    "    #model.add (tf.keras.layers.Dropout(1/3))\n",
    "\n",
    "    # CG: add an output layer with output units for all NUM_CATEGORIES categories:\n",
    "    model.add (tf.keras.layers.Dense(NUM_CATEGORIES, activation=\"softmax\"))\n",
    "\n",
    "    # CG: compile the model\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    # CG: return the compiled model:\n",
    "    return model\n",
    "  \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47e04d54-f645-4a53-9346-6d2c4f6142a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35985bb4-1f99-4795-89fd-5e7c257d4d89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
