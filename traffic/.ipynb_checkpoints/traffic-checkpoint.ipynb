{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3162626b-5311-45ae-b621-8c50d16a4c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 43/43 [00:03<00:00, 11.68it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"max_pooling2d_12\" (type MaxPooling2D).\n\nNegative dimension size caused by subtracting 2 from 1 for '{{node max_pooling2d_12/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", explicit_paddings=[], ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1]](Placeholder)' with input shapes: [?,1,1,32].\n\nCall arguments received by layer \"max_pooling2d_12\" (type MaxPooling2D):\n  • inputs=tf.Tensor(shape=(None, 1, 1, 32), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 180>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 181\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [31]\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m x_train, x_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m     32\u001b[0m     np\u001b[38;5;241m.\u001b[39marray(images), np\u001b[38;5;241m.\u001b[39marray(labels), test_size\u001b[38;5;241m=\u001b[39mTEST_SIZE\n\u001b[0;32m     33\u001b[0m )\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Get a compiled neural network\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Fit model on training data\u001b[39;00m\n\u001b[0;32m     39\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(x_train, y_train, epochs\u001b[38;5;241m=\u001b[39mEPOCHS)\n",
      "Input \u001b[1;32mIn [31]\u001b[0m, in \u001b[0;36mget_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03mReturns a compiled convolutional neural network model. Assume that the\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;124;03m`input_shape` of the first layer is `(IMG_WIDTH, IMG_HEIGHT, 3)`.\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;124;03mThe output layer should have `NUM_CATEGORIES` units, one for each category.\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# CG: create the model:\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSequential\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m    131\u001b[0m \n\u001b[0;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# CG: create a convolutional layer. Learn 128 filters using a 4x4 kernel, shaping input according to given parameters:\u001b[39;49;00m\n\u001b[0;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2D\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mIMG_WIDTH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIMG_HEIGHT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \n\u001b[0;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# CG: max-pooling layer, using 4x4 pool size:\u001b[39;49;00m\n\u001b[0;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMaxPooling2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \n\u001b[0;32m    140\u001b[0m \n\u001b[0;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# CG: create a convolutional layer. Learn 128 filters using a 4x4 kernel, shaping input according to given parameters:\u001b[39;49;00m\n\u001b[0;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2D\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mIMG_WIDTH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIMG_HEIGHT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \n\u001b[0;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# CG: max-pooling layer, using 4x4 pool size:\u001b[39;49;00m\n\u001b[0;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMaxPooling2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \n\u001b[0;32m    149\u001b[0m \n\u001b[0;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# CG: create a convolutional layer. Learn 32 filters using a 3x3 kernel, shaping input according to given parameters:\u001b[39;49;00m\n\u001b[0;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#tf.keras.layers.Conv2D(\u001b[39;49;00m\n\u001b[0;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#    64, (3, 3), activation=\"relu\", input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)\u001b[39;49;00m\n\u001b[0;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#),\u001b[39;49;00m\n\u001b[0;32m    154\u001b[0m \n\u001b[0;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# CG: max-pooling layer, using 2x2 pool size:\u001b[39;49;00m\n\u001b[0;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\u001b[39;49;00m\n\u001b[0;32m    157\u001b[0m \n\u001b[0;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# CG: flatten units:\u001b[39;49;00m\n\u001b[0;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFlatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# CG: add a hidden layer with 50% dropout:\u001b[39;49;00m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \n\u001b[0;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# CG: add an output layer with output units for all NUM_CATEGORIES categories:\u001b[39;49;00m\n\u001b[0;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mNUM_CATEGORIES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msoftmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# CG: compile the model\u001b[39;00m\n\u001b[0;32m    170\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m    171\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    172\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    173\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    174\u001b[0m )\n",
      "File \u001b[1;32mD:\\Documents\\Anaconda3\\envs\\ai50\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py:587\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    586\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 587\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    589\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Documents\\Anaconda3\\envs\\ai50\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mD:\\Documents\\Anaconda3\\envs\\ai50\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1963\u001b[0m, in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[0;32m   1960\u001b[0m   c_op \u001b[38;5;241m=\u001b[39m pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_FinishOperation(op_desc)\n\u001b[0;32m   1961\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mInvalidArgumentError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1962\u001b[0m   \u001b[38;5;66;03m# Convert to ValueError for backwards compatibility.\u001b[39;00m\n\u001b[1;32m-> 1963\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(e\u001b[38;5;241m.\u001b[39mmessage)\n\u001b[0;32m   1965\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m c_op\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"max_pooling2d_12\" (type MaxPooling2D).\n\nNegative dimension size caused by subtracting 2 from 1 for '{{node max_pooling2d_12/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", explicit_paddings=[], ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1]](Placeholder)' with input shapes: [?,1,1,32].\n\nCall arguments received by layer \"max_pooling2d_12\" (type MaxPooling2D):\n  • inputs=tf.Tensor(shape=(None, 1, 1, 32), dtype=float32)"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "EPOCHS = 10\n",
    "IMG_WIDTH = 30\n",
    "IMG_HEIGHT = 30\n",
    "NUM_CATEGORIES = 43\n",
    "TEST_SIZE = 0.4\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Check command-line arguments\n",
    "    #if len(sys.argv) not in [2, 3]:\n",
    "    #    sys.exit(\"Usage: python traffic.py data_directory [model.h5]\")\n",
    "\n",
    "    # Get image arrays and labels for all image files\n",
    "    #images, labels = load_data(sys.argv[1])\n",
    "    sys_argv1 = \"gtsrb\"\n",
    "    images, labels = load_data(sys_argv1)\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    labels = tf.keras.utils.to_categorical(labels)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        np.array(images), np.array(labels), test_size=TEST_SIZE\n",
    "    )\n",
    "\n",
    "    # Get a compiled neural network\n",
    "    model = get_model()\n",
    "\n",
    "    # Fit model on training data\n",
    "    model.fit(x_train, y_train, epochs=EPOCHS)\n",
    "\n",
    "    # Evaluate neural network performance\n",
    "    model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "    # Save model to file\n",
    "    #if len(sys.argv) == 3:\n",
    "    #    filename = sys.argv[2]\n",
    "    #    model.save(filename)\n",
    "    #    print(f\"Model saved to {filename}.\")\n",
    "    sys_argv2 = \"model.sav\"\n",
    "    filename = sys_argv2\n",
    "    model.save(filename)\n",
    "    print(f\"Model saved to {filename}.\")\n",
    "\n",
    "\n",
    "def load_data(data_dir):\n",
    "    \"\"\"\n",
    "    Load image data from directory `data_dir`.\n",
    "\n",
    "    Assume `data_dir` has one directory named after each category, numbered\n",
    "    0 through NUM_CATEGORIES - 1. Inside each category directory will be some\n",
    "    number of image files.\n",
    "\n",
    "    Return tuple `(images, labels)`. `images` should be a list of all\n",
    "    of the images in the data directory, where each image is formatted as a\n",
    "    numpy ndarray with dimensions IMG_WIDTH x IMG_HEIGHT x 3. `labels` should\n",
    "    be a list of integer labels, representing the categories for each of the\n",
    "    corresponding `images`.\n",
    "    \"\"\"\n",
    "    # CG: initiate resulting lists:\n",
    "    images = list()\n",
    "    labels = list()\n",
    "    filenames = list()\n",
    "    imagelist = list()\n",
    "    badreads  = list()\n",
    "\n",
    "    #CG: assume that data_dir will contain one directory named after each category, numbered 0 through NUM_CATEGORIES - 1:\n",
    "    for cat in tqdm(range(NUM_CATEGORIES)):\n",
    "\n",
    "        # CG: build search_dir by adding the category number to the starting dir:\n",
    "        search_dir = data_dir + os.sep + str(cat) + os.sep\n",
    "\n",
    "        # CG: walk down the directory tree:\n",
    "        for root, dirs, files in os.walk(search_dir):\n",
    "\n",
    "            # CG: get a list of all files in the directory:\n",
    "            names = [(os.path.join(root,f)) for f in files]\n",
    "\n",
    "            # CG: ignore empty entries:\n",
    "            if len(names) == 0:\n",
    "                continue\n",
    "\n",
    "            # CG: add an entry in the arrays for each file found:\n",
    "            for f in names:\n",
    "\n",
    "                # CG: load the image file:\n",
    "                img = cv2.imread (f, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "                # CG: if failed, add to badreads list: \n",
    "                if img is None:\n",
    "                    badreads.append (f)\n",
    "                    continue\n",
    "\n",
    "                # CG: add filename to list of valid files:\n",
    "                filenames.append(f)\n",
    "\n",
    "                # CG: resize the image to the desired sizes:\n",
    "                img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
    "\n",
    "                # CG: convert the image to RGB color scheme:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                # CG: add the image to the list:\n",
    "                images.append(img)\n",
    "\n",
    "                # CG: add the corresponding categorie to the list:\n",
    "                labels.append(cat)\n",
    "\n",
    "    # CG: return the lists:\n",
    "    return (images, labels)\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    \"\"\"\n",
    "    Returns a compiled convolutional neural network model. Assume that the\n",
    "    `input_shape` of the first layer is `(IMG_WIDTH, IMG_HEIGHT, 3)`.\n",
    "    The output layer should have `NUM_CATEGORIES` units, one for each category.\n",
    "    \"\"\"\n",
    "\n",
    "    # CG: create the model:\n",
    "    model = tf.keras.models.Sequential([\n",
    "\n",
    "        # CG: create a convolutional layer. Learn 128 filters using a 4x4 kernel, shaping input according to given parameters:\n",
    "        tf.keras.layers.Conv2D(\n",
    "            128, (4, 4), activation=\"relu\", input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)\n",
    "        ),\n",
    "\n",
    "        # CG: max-pooling layer, using 4x4 pool size:\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(8, 8)),\n",
    "\n",
    "\n",
    "        # CG: create a convolutional layer. Learn 128 filters using a 4x4 kernel, shaping input according to given parameters:\n",
    "        tf.keras.layers.Conv2D(\n",
    "            32, (3, 3), activation=\"relu\", input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)\n",
    "        ),\n",
    "\n",
    "        # CG: max-pooling layer, using 4x4 pool size:\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "\n",
    "        # CG: create a convolutional layer. Learn 32 filters using a 3x3 kernel, shaping input according to given parameters:\n",
    "        #tf.keras.layers.Conv2D(\n",
    "        #    64, (3, 3), activation=\"relu\", input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)\n",
    "        #),\n",
    "\n",
    "        # CG: max-pooling layer, using 2x2 pool size:\n",
    "        #tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        # CG: flatten units:\n",
    "        tf.keras.layers.Flatten(),\n",
    "\n",
    "        # CG: add a hidden layer with 50% dropout:\n",
    "        tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(1/3),\n",
    "\n",
    "        # CG: add an output layer with output units for all NUM_CATEGORIES categories:\n",
    "        tf.keras.layers.Dense(NUM_CATEGORIES, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    # CG: compile the model\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    # CG: return the compiled model:\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641bdde3-c837-438e-bafc-8674aeec5578",
   "metadata": {},
   "outputs": [],
   "source": [
    "333/333 - 3s - loss: 0.2601 - accuracy: 0.9361 - 3s/epoch - 8ms/step\n",
    "        # CG: create a convolutional layer. Learn 32 filters using a 4x4 kernel, shaping input according to given parameters:\n",
    "        tf.keras.layers.Conv2D(\n",
    "            128, (4, 4), activation=\"relu\", input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)\n",
    "        ),\n",
    "\n",
    "        # CG: max-pooling layer, using 4x4 pool size:\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(4, 4)),\n",
    "        \n",
    "        # CG: add a hidden layer with dropout:\n",
    "        tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47922c9f-1fc1-434b-88d1-d760488ee356",
   "metadata": {},
   "outputs": [],
   "source": [
    "333/333 - 3s - loss: 0.3942 - accuracy: 0.9217 - 3s/epoch - 8ms/step\n",
    "    # CG: create the model:\n",
    "    model = tf.keras.models.Sequential([\n",
    "\n",
    "        # CG: create a convolutional layer. Learn 32 filters using a 4x4 kernel, shaping input according to given parameters:\n",
    "        tf.keras.layers.Conv2D(\n",
    "            128, (4, 4), activation=\"relu\", input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)\n",
    "        ),\n",
    "\n",
    "        # CG: max-pooling layer, using 4x4 pool size:\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(4, 4)),\n",
    "\n",
    "\n",
    "        # CG: create a convolutional layer. Learn 32 filters using a 3x3 kernel, shaping input according to given parameters:\n",
    "        #tf.keras.layers.Conv2D(\n",
    "        #    64, (3, 3), activation=\"relu\", input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)\n",
    "        #),\n",
    "\n",
    "        # CG: max-pooling layer, using 2x2 pool size:\n",
    "        #tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        # CG: flatten units:\n",
    "        tf.keras.layers.Flatten(),\n",
    "\n",
    "        # CG: add a hidden layer with 50% dropout:\n",
    "        tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(1/3),\n",
    "\n",
    "        # CG: add an output layer with output units for all NUM_CATEGORIES categories:\n",
    "        tf.keras.layers.Dense(NUM_CATEGORIES, activation=\"softmax\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b15d196-7ef1-4652-a50f-16e901d88a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Negative dimension size caused by subtracting 2 from 1 for '{{node max_pooling2d_12/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", explicit_paddings=[], ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1]](Placeholder)' with input shapes: [?,1,1,32].\n",
    "\n",
    "Call arguments received by layer \"max_pooling2d_12\" (type MaxPooling2D):\n",
    "  • inputs=tf.Tensor(shape=(None, 1, 1, 32), dtype=float32)\n",
    "\n",
    "    # CG: create the model:\n",
    "    model = tf.keras.models.Sequential([\n",
    "\n",
    "        # CG: create a convolutional layer. Learn 128 filters using a 4x4 kernel, shaping input according to given parameters:\n",
    "        tf.keras.layers.Conv2D(\n",
    "            128, (4, 4), activation=\"relu\", input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)\n",
    "        ),\n",
    "\n",
    "        # CG: max-pooling layer, using 4x4 pool size:\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(8, 8)),\n",
    "\n",
    "\n",
    "        # CG: create a convolutional layer. Learn 128 filters using a 4x4 kernel, shaping input according to given parameters:\n",
    "        tf.keras.layers.Conv2D(\n",
    "            32, (3, 3), activation=\"relu\", input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)\n",
    "        ),\n",
    "\n",
    "        # CG: max-pooling layer, using 4x4 pool size:\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "\n",
    "        # CG: create a convolutional layer. Learn 32 filters using a 3x3 kernel, shaping input according to given parameters:\n",
    "        #tf.keras.layers.Conv2D(\n",
    "        #    64, (3, 3), activation=\"relu\", input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)\n",
    "        #),\n",
    "\n",
    "        # CG: max-pooling layer, using 2x2 pool size:\n",
    "        #tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        # CG: flatten units:\n",
    "        tf.keras.layers.Flatten(),\n",
    "\n",
    "        # CG: add a hidden layer with 50% dropout:\n",
    "        tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(1/3),\n",
    "\n",
    "        # CG: add an output layer with output units for all NUM_CATEGORIES categories:\n",
    "        tf.keras.layers.Dense(NUM_CATEGORIES, activation=\"softmax\")\n",
    "    ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
