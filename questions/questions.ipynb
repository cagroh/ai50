{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1b81e39a-1f5e-4767-b28f-43932a9f3b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Query:  What are the types of supervised learning?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types of supervised learning algorithms include Active learning , classification and regression.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import sys\n",
    "import os\n",
    "import string\n",
    "import math\n",
    "\n",
    "FILE_MATCHES = 1\n",
    "SENTENCE_MATCHES = 1\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Check command-line arguments\n",
    "    #if len(sys.argv) != 2:\n",
    "    #    sys.exit(\"Usage: python questions.py corpus\")\n",
    "\n",
    "    # Calculate IDF values across files\n",
    "    #files = load_files(sys.argv[1])\n",
    "    files = load_files('corpus')\n",
    "    file_words = {\n",
    "        filename: tokenize(files[filename])\n",
    "        for filename in files\n",
    "    }\n",
    "    file_idfs = compute_idfs(file_words)\n",
    "\n",
    "    # Prompt user for query\n",
    "    query = set(tokenize(input(\"Query: \")))\n",
    "\n",
    "    # Determine top file matches according to TF-IDF\n",
    "    filenames = top_files(query, file_words, file_idfs, n=FILE_MATCHES)\n",
    "\n",
    "    # Extract sentences from top files\n",
    "    sentences = dict()\n",
    "    for filename in filenames:\n",
    "        for passage in files[filename].split(\"\\n\"):\n",
    "            for sentence in nltk.sent_tokenize(passage):\n",
    "                tokens = tokenize(sentence)\n",
    "                if tokens:\n",
    "                    sentences[sentence] = tokens\n",
    "\n",
    "    # Compute IDF values across sentences\n",
    "    idfs = compute_idfs(sentences)\n",
    "\n",
    "    # Determine top sentence matches\n",
    "    matches = top_sentences(query, sentences, idfs, n=SENTENCE_MATCHES)\n",
    "    for match in matches:\n",
    "        print(match)\n",
    "\n",
    "\n",
    "def load_files(directory):\n",
    "    \"\"\"\n",
    "    Given a directory name, return a dictionary mapping the filename of each\n",
    "    `.txt` file inside that directory to the file's contents as a string.\n",
    "    \"\"\"\n",
    "    # CG: initialize resulting dictionary:\n",
    "    result = dict()\n",
    "\n",
    "    # CG: build search_dir by adding the category number to the starting dir:\n",
    "    search_dir = directory + os.sep\n",
    "\n",
    "    # CG: walk down the directory tree:\n",
    "    for root, dirs, files in os.walk(search_dir):\n",
    "\n",
    "        # CG: get a list of all files in the directory:\n",
    "        filenames = [(os.path.join(root, file)) for file in files]\n",
    "\n",
    "        # CG: ignore empty entries:\n",
    "        if len(filenames) == 0:\n",
    "            continue\n",
    "\n",
    "        # CG: add an entry in the arrays for each file found:\n",
    "        for i in range(len(filenames)):\n",
    "\n",
    "            # CG: open the file:\n",
    "            f = open(filenames[i], mode='r', encoding='utf-8')\n",
    "            \n",
    "            #CG: read the content:\n",
    "            contents = f.read()\n",
    "            \n",
    "            result.update ({files[i]: contents})\n",
    "\n",
    "            # CG: close the file:\n",
    "            f.close()\n",
    "\n",
    "    # CG: return the resulting dictionary:\n",
    "    return result\n",
    "\n",
    "\n",
    "def tokenize(document):\n",
    "    \"\"\"\n",
    "    Given a document (represented as a string), return a list of all of the\n",
    "    words in that document, in order.\n",
    "\n",
    "    Process document by coverting all words to lowercase, and removing any\n",
    "    punctuation or English stopwords.\n",
    "    \"\"\"\n",
    "    # CG: helper to extract and tokenize the words from the document, lowercase and rid of punctuation:\n",
    "    #     (adapted from sentiment.py)\n",
    "    def extract_words(document):\n",
    "        return list(\n",
    "            word.translate({ord(i): None for i in string.punctuation}).lower() for word in nltk.word_tokenize(document)\n",
    "            if any(c.isalpha() for c in word)\n",
    "        )\n",
    "\n",
    "    # CG: initialize resulting list:\n",
    "    result = list()\n",
    "\n",
    "    # CG: extract and tokenize the words from the document, lowercase and rid of punctuation:\n",
    "    tokenized_document = extract_words(document)\n",
    "\n",
    "    # CG: iterate thru all words in the tokenized document:\n",
    "    for word in tokenized_document:\n",
    "\n",
    "        # CG: test if the word is not a stop word:\n",
    "        if word not in nltk.corpus.stopwords.words(\"english\"):\n",
    "\n",
    "            # CG: add the word to the list:\n",
    "            result.append (word)\n",
    "\n",
    "    # CG: sort the list:\n",
    "    result = (sorted(result))\n",
    "\n",
    "    # CG: return the result:\n",
    "    return result\n",
    "\n",
    "\n",
    "def compute_idfs(documents):\n",
    "    \"\"\"\n",
    "    Given a dictionary of `documents` that maps names of documents to a list\n",
    "    of words, return a dictionary that maps words to their IDF values.\n",
    "\n",
    "    Any word that appears in at least one of the documents should be in the\n",
    "    resulting dictionary.\n",
    "    \"\"\"\n",
    "    # CG: initialize a set of unique words:\n",
    "    words_set = set()\n",
    "\n",
    "    # CG: initialize dictionary of documents and words with their respective count inside each document:\n",
    "    word_doc_count = dict()\n",
    "\n",
    "    # CG: loop over all documents to come-up with a set of unique words and a dictionary with word and it's frequence inside a document:\n",
    "    for document in documents:\n",
    "\n",
    "        # CG: initialize subdictionary of words and their respective count inside the document:\n",
    "        word_occurrences = dict()\n",
    "\n",
    "        # CG: add the entire set of words of the document to the set of unique words:\n",
    "        words_set.update(documents[document])\n",
    "\n",
    "        # CG: loop over all words in the document:\n",
    "        for word in documents[document]:\n",
    "\n",
    "            # CG: check if the word already exists in the list:\n",
    "            if word in word_occurrences:\n",
    "\n",
    "                # CG: compute one more occurence:\n",
    "                word_occurrences[word] += 1\n",
    "            else:\n",
    "\n",
    "                # CG: first occurence:\n",
    "                word_occurrences[word]  = 1\n",
    "\n",
    "        # CG: add the subdictionary to the corpus:\n",
    "        word_doc_count[document] = word_occurrences\n",
    "\n",
    "    # CG: the following code was adapted from tfidf.py: \n",
    "    # CG: initialize dictionary of idfs:\n",
    "    idfs = dict()\n",
    "    \n",
    "    # CG: iterate thru all unique words:\n",
    "    for word in words_set:\n",
    "        \n",
    "        # CG: compute word total frequency:\n",
    "        f = sum(word in word_doc_count[docname] for docname in word_doc_count)\n",
    "\n",
    "        # CG: compute IDFs\n",
    "        idf = math.log(len(word_doc_count) / f)\n",
    "        \n",
    "        # CG: save the IDF for the word:\n",
    "        idfs[word] = idf\n",
    "\n",
    "    return idfs\n",
    "\n",
    "\n",
    "def top_files(query, files, idfs, n):\n",
    "    \"\"\"\n",
    "    Given a `query` (a set of words), `files` (a dictionary mapping names of\n",
    "    files to a list of their words), and `idfs` (a dictionary mapping words\n",
    "    to their IDF values), return a list of the filenames of the the `n` top\n",
    "    files that match the query, ranked according to tf-idf.\n",
    "    \"\"\"\n",
    "    # CG: let's signal some activity:\n",
    "    #print (f\"Searching for terms inside documents...\")\n",
    "\n",
    "    # CG: initialize dictionary to store occurrences of a word inside a file:\n",
    "    occurrences = dict()\n",
    "\n",
    "    # CG: loop over all words in the query:\n",
    "    for word in query:\n",
    "\n",
    "        # CG: loop over all files:\n",
    "        for file in files:\n",
    "\n",
    "            # CG: compute the occurences of a word in a file:\n",
    "            occurrences[word+'|'+file]=list(files[file]).count(word)\n",
    "\n",
    "    # CG: the following code was adapted from tfidf.py:\n",
    "    # CG: initialize dictionary of files and their respective total TF-IDF:\n",
    "    dtfidfs = dict()\n",
    "\n",
    "    # CG: loop over all files:\n",
    "    for filename in files:\n",
    "\n",
    "        # CG: initialize total TF-IDF count:\n",
    "        tfidfs = 0\n",
    "\n",
    "        # CG: loop over all words in the query:\n",
    "        for word in query:\n",
    "\n",
    "            # CG: compute word frequency inside a file:\n",
    "            tf = occurrences[word+'|'+filename]\n",
    "\n",
    "            # CG: accumulate total TF-IDF of this file:\n",
    "            tfidfs += tf * idfs[word]\n",
    "\n",
    "        # CG: store file's TF-IDF:\n",
    "        dtfidfs[filename] = tfidfs\n",
    "\n",
    "    # CG: sort the resulting dictionary:\n",
    "    dtfidfs = dict(sorted(dtfidfs.items(), key=lambda row: row[1], reverse=True))\n",
    "\n",
    "    # print (dtfidfs)\n",
    "    \n",
    "    # CG: initialize resulting list:\n",
    "    result = list()\n",
    "\n",
    "    # CG: build the resuling list, topped to n elements:\n",
    "    result = list(dtfidfs.keys())[:n]\n",
    "    \n",
    "    # CG: return the resulting list:\n",
    "    return result\n",
    "\n",
    "\n",
    "def top_sentences(query, sentences, idfs, n):\n",
    "    \"\"\"\n",
    "    Given a `query` (a set of words), `sentences` (a dictionary mapping\n",
    "    sentences to a list of their words), and `idfs` (a dictionary mapping words\n",
    "    to their IDF values), return a list of the `n` top sentences that match\n",
    "    the query, ranked according to idf. If there are ties, preference should\n",
    "    be given to sentences that have a higher query term density.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Query term density is defined as the proportion of words in the sentence that \n",
    "    are also words in the query. For example, if a sentence has 10 words, 3 of which \n",
    "    are in the query, then the sentence’s query term density is 0.3.\n",
    "    \"\"\"\n",
    "    # CG: initialize dictionary of sentences and their respective total IDF:\n",
    "    sentence_idf = dict()\n",
    "\n",
    "    # CG: loop over all sentences:\n",
    "    for sentence in sentences:\n",
    "\n",
    "        # CG: initialize list of unique occurrencies of query words found in the sentence:\n",
    "        words_found  = list()\n",
    "\n",
    "        # CG: initialize total TF-IDF count:\n",
    "        sum_idfs = 0\n",
    "\n",
    "        # CG: loop over all words in the query:\n",
    "        for word in query:\n",
    "\n",
    "            # CG: check if a word is in the sentence:\n",
    "            if word in sentences[sentence]:\n",
    "\n",
    "                # CG: accumulate total IDF of this sentence:\n",
    "                sum_idfs += idfs[word]\n",
    "\n",
    "        # CG: loop over all word in the sentence:\n",
    "        for word in sentences[sentence]:\n",
    "\n",
    "            # CG: check if the word is in the query:\n",
    "            if word in query:\n",
    "\n",
    "                    # CG: add the word to the list:\n",
    "                    words_found.append (word)\n",
    "\n",
    "        # CG: store sentence's IDF alongside with its query term density in the dictionary:\n",
    "        sentence_idf[sentence] = (sum_idfs, len(words_found) / len(sentences[sentence]))\n",
    "\n",
    "    # CG: sort the resulting dictionary:\n",
    "    sentence_idf = dict(sorted(sentence_idf.items(), key = lambda x: (x[1][0], x[1][1]), reverse=True))\n",
    "\n",
    "    # CG: initialize resulting list:\n",
    "    result = list()\n",
    "\n",
    "    # CG: build the resuling list, topped to n elements:\n",
    "    result = list(sentence_idf.keys())[:n]\n",
    "\n",
    "    # CG: return the resulting list:\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c19707-45e5-47b4-b3f8-bab8c1401127",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
